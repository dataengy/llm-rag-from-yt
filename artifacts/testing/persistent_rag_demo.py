#!/usr/bin/env python3
"""Persistent RAG demo with ChromaDB storage."""

import os
import sys
import yaml
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

from loguru import logger as log
from bash import bash

# Import common utilities
from utils import (
    setup_python_path, setup_logging, load_config, load_transcription_text,
    create_rag_metadata, print_session_header, print_section_header, print_test_results
)

# Setup Python path and logging
setup_python_path()
log_file = setup_logging('persistent_rag_demo')

def create_persistent_rag_system(config: dict):
    """Create a persistent RAG system using ChromaDB."""
    
    log.info("Starting persistent RAG system creation")
    print("üîß Creating persistent RAG system with ChromaDB...")
    
    # Load transcription
    log.debug("Loading transcription text")
    transcript_file = Path(config['data']['transcript_file'])
    transcript_text = load_transcription_text(transcript_file)
    if not transcript_text:
        log.error("No transcript text loaded, aborting RAG creation")
        return None
    
    log.info(f"Transcript loaded successfully: {len(transcript_text)} characters")
    
    try:
        log.debug("Importing project components")
        from llm_rag_yt.text.processor import TextProcessor
        from llm_rag_yt.embeddings.encoder import EmbeddingEncoder
        from llm_rag_yt.vectorstore.chroma import ChromaVectorStore
        
        log.info("Project components imported successfully")
        print("‚úÖ Project components available")
        
        # Create text processor
        log.debug("Creating TextProcessor instance")
        text_processor = TextProcessor()
        
        # Process the transcript
        log.info("Starting transcript processing")
        print("üìù Processing transcript into chunks...")
        
        log.debug("Normalizing transcript text")
        normalized_text = text_processor.normalize_text(transcript_text)
        log.debug(f"Normalized text length: {len(normalized_text)} characters")
        
        chunk_size = config['text_processing']['chunk_size']
        chunk_overlap = config['text_processing']['chunk_overlap']
        
        log.debug(f"Splitting into chunks with size={chunk_size}, overlap={chunk_overlap}")
        text_chunks = text_processor.split_into_chunks(
            normalized_text,
            chunk_size=chunk_size,
            overlap=chunk_overlap
        )
        
        log.info(f"Successfully created {len(text_chunks)} text chunks")
        print(f"‚úÖ Created {len(text_chunks)} text chunks")
        
        # Create chunks with metadata
        log.debug("Creating chunk objects with metadata")
        chunks = []
        for i, text in enumerate(text_chunks):
            chunk = {
                "id": f"transcript_chunk_{i}",
                "text": text,
                "metadata": {
                    "source": config['metadata']['source'],
                    "chunk_index": i,
                    "title": config['metadata']['title'],
                    "language": config['metadata']['language'],
                    "created_at": datetime.now().isoformat()
                }
            }
            chunks.append(chunk)
        
        log.info(f"Created {len(chunks)} chunk objects with metadata")
        
        # Initialize ChromaDB
        log.info("Initializing ChromaDB vector store")
        print("üóÑÔ∏è Initializing ChromaDB vector store...")
        
        chroma_path = Path(config['chroma_db']['path'])
        collection_name = config['chroma_db']['collection_name']
        
        vector_store = ChromaVectorStore(
            persist_dir=chroma_path,
            collection_name=collection_name
        )
        
        log.info(f"ChromaDB initialized at: {chroma_path}")
        print(f"‚úÖ ChromaDB initialized: {chroma_path}")
        
        # Initialize embedding encoder
        log.info("Initializing embedding encoder")
        print("üß† Loading embedding model...")
        
        embedding_model = config['embeddings']['model']
        encoder = EmbeddingEncoder(model_name=embedding_model)
        
        log.info(f"Embedding model loaded: {embedding_model}")
        print(f"‚úÖ Embedding model loaded: {embedding_model}")
        
        # Store chunks in ChromaDB
        log.info("Storing chunks in ChromaDB")
        print("üíæ Storing chunks in vector database...")
        
        vector_store.upsert_chunks(encoder, chunks)
        
        log.info("Chunks stored successfully in ChromaDB")
        print(f"‚úÖ Stored {len(chunks)} chunks in ChromaDB")
        
        # Get collection info
        collection_info = vector_store.get_collection_info()
        log.info(f"Collection info: {collection_info}")
        print(f"üìä Collection: {collection_info['name']} ({collection_info['count']} documents)")
        
        # Create RAG system data
        rag_system = {
            "vector_store": vector_store,
            "encoder": encoder,
            "chunks": chunks,
            "full_text": transcript_text,
            "metadata": create_rag_metadata(
                source=config['metadata']['source'],
                title=config['metadata']['title'],
                language=config['metadata']['language'],
                chunk_count=len(chunks),
                transcript_length=len(transcript_text),
                method="persistent_chromadb"
            )
        }
        
        # Add additional metadata
        rag_system['metadata'].update({
            "embedding_model": embedding_model,
            "chroma_path": str(chroma_path),
            "collection_name": collection_name
        })
        
        log.info("Persistent RAG system created successfully")
        log.debug(f"RAG system metadata: {rag_system['metadata']}")
        
        return rag_system
        
    except ImportError as e:
        log.error(f"Project components not available: {e}")
        print(f"‚ùå Project components not available: {e}")
        return None
    except Exception as e:
        log.error(f"Error creating persistent RAG system: {e}")
        print(f"‚ùå Error creating RAG system: {e}")
        return None

def semantic_search(query: str, rag_system: Dict[str, Any], top_k: int = 3) -> List[Dict[str, Any]]:
    """Perform semantic search using ChromaDB embeddings."""
    
    log.debug(f"Starting semantic search for query: '{query}'")
    
    try:
        # Embed the query
        log.debug("Embedding query")
        query_embedding = rag_system['encoder'].embed_query(query)
        log.debug(f"Query embedded successfully")
        
        # Search in vector store
        log.debug(f"Searching ChromaDB for top {top_k} results")
        results = rag_system['vector_store'].query_similar(query_embedding, top_k=top_k)
        
        log.info(f"Semantic search completed: {len(results)} results found for query '{query}'")
        
        for i, result in enumerate(results):
            log.debug(f"Result {i+1}: distance={result.get('distance', 'N/A'):.3f}, text='{result['text'][:50]}...'")
        
        return results
        
    except Exception as e:
        log.error(f"Error in semantic search: {e}")
        return []

def generate_rag_answer(question: str, search_results: List[Dict[str, Any]], full_text: str) -> str:
    """Generate answer using semantic search results."""
    
    log.debug(f"Generating RAG answer for question: '{question}'")
    log.debug(f"Search results available: {len(search_results)}")
    
    if not search_results:
        log.warning("No search results available for answer generation")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
    
    # Extract context from search results
    context_chunks = [result['text'] for result in search_results]
    context = " ".join(context_chunks)
    log.debug(f"Combined context length: {len(context)} characters")
    
    # Log search result distances for analysis
    distances = [result.get('distance', 1.0) for result in search_results]
    log.debug(f"Search result distances: {distances}")
    
    # Common question patterns with enhanced logic
    if any(word in question.lower() for word in ["—Å–∫–æ–ª—å–∫–æ", "–∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç", "–∑–∞—Ä–∞–±–æ—Ç–æ–∫", "–¥–µ–Ω—å–≥–∏", "—Ä—É–±–ª–µ–π"]):
        log.debug("Detected earnings-related question")
        if "–º–∏–ª–ª–∏–æ–Ω" in context.lower():
            answer = "–ü–æ —Å–ª–æ–≤–∞–º –≥–µ—Ä–æ—è –≤–∏–¥–µ–æ, –æ–Ω –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–µ –º–∏–ª–ª–∏–æ–Ω–∞ —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü. –û–Ω –æ–±—ä—è—Å–Ω—è–µ—Ç, —á—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ - –∞–∫—Ç–µ—Ä—Å—Ç–≤–æ, —Å—Ü–µ–Ω–∞—Ä–Ω–æ–µ –¥–µ–ª–æ, –∫–æ–Ω—Ü–µ—Ä—Ç—ã, –±–ª–æ–≥–∏–Ω–≥, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ."
            log.info(f"Generated earnings answer: '{answer[:50]}...'")
            return answer
    
    if any(word in question.lower() for word in ["—á—Ç–æ", "—á–µ–º", "—Ä–∞–±–æ—Ç–∞", "–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å"]):
        log.debug("Detected work/activity-related question")
        if "–∞–∫—Ç–µ—Ä" in context.lower() or "—Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç" in context.lower():
            answer = "–ì–µ—Ä–æ–π –≤–∏–¥–µ–æ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–Ω —Ç—Ä—É–¥–æ–≥–æ–ª–∏–∫ –∏ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–∫—Ç–µ—Ä—Å—Ç–≤–æ, —Å—Ü–µ–Ω–∞—Ä–Ω–æ–µ –¥–µ–ª–æ, –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π, –∫–æ–Ω—Ü–µ—Ä—Ç—ã, –±–ª–æ–≥–∏–Ω–≥, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –¥—Ä—É–≥–æ–µ."
            log.info(f"Generated activity answer: '{answer[:50]}...'")
            return answer
    
    if any(word in question.lower() for word in ["–∫–∞–∫", "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º"]):
        log.debug("Detected how/method-related question")
        answer = "–ì–µ—Ä–æ–π –≤–∏–¥–µ–æ –æ–±—ä—è—Å–Ω—è–µ—Ç, —á—Ç–æ —Å–∞–º–∞—è –±–æ–ª—å—à–∞—è –ø—Ä–æ–±–ª–µ–º–∞ - —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å, —á–µ–º –∏–º–µ–Ω–Ω–æ –æ–Ω –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Å–µ–≥–æ. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –∞–∫—Ç–µ—Ä, —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç, –ø—Ä–æ–≤–æ–¥–∏—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è, –≤—ã—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç–∞—Ö, –≤–µ–¥–µ—Ç –±–ª–æ–≥ –∏ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è–º–∏."
        log.info(f"Generated method answer: '{answer[:50]}...'")
        return answer
    
    # Default response with most relevant context
    log.debug("Using semantic context-based answer")
    best_match = search_results[0]['text'] if search_results else ""
    answer = f"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {best_match[:200]}..."
    log.info(f"Generated semantic answer: '{answer[:50]}...'")
    return answer

def interactive_persistent_qa(rag_system: Dict[str, Any], config: dict):
    """Run interactive Q&A session with persistent storage."""
    
    log.info("Starting interactive persistent Q&A session")
    log.debug(f"RAG system contains vector store with {rag_system['metadata']['chunk_count']} chunks")
    
    print("
" + "="*70)
    print("ü§ñ PERSISTENT RAG Q&A - –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö –° CHROMADB")
    print("="*70)
    print(f"üìπ –í–∏–¥–µ–æ: {rag_system['metadata']['title']}")
    print(f"üìä –î–∞–Ω–Ω—ã–µ: {rag_system['metadata']['chunk_count']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB")
    print(f"üß† –ú–æ–¥–µ–ª—å: {rag_system['metadata']['embedding_model']}")
    print(f"üóÑÔ∏è –ë–∞–∑–∞: {rag_system['metadata']['chroma_path']}")
    print("üí° –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –≤–∏–¥–µ–æ")
    print("‚å®Ô∏è –ù–∞–ø–∏—à–∏—Ç–µ 'quit' –∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print("="*70)
    
    max_results = config['search']['max_results']
    
    while True:
        try:
            log.debug("Waiting for user input...")
            question = input("
ü§î –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            
            if question.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                log.info("User requested to quit interactive session")
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not question:
                log.debug("Empty question received")
                print("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")
                continue
            
            log.info(f"Processing user question: '{question}'")
            print(f"
üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è: '{question}'")
            
            # Perform semantic search
            log.debug("Starting semantic search")
            search_results = semantic_search(question, rag_system, top_k=max_results)
            
            # Generate answer
            log.debug("Starting answer generation")
            answer = generate_rag_answer(question, search_results, rag_system["full_text"])
            
            log.info(f"Generated answer for question '{question}': '{answer[:50]}...'")
            
            print(f"
üí¨ –û–¢–í–ï–¢:")
            print("-" * 50)
            print(answer)
            
            if search_results:
                log.debug(f"Displaying {len(search_results)} search results")
                print(f"
üìö –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò –ü–û–•–û–ñ–ò–ï –§–†–ê–ì–ú–ï–ù–¢–´ ({len(search_results)}):")
                print("-" * 50)
                for i, result in enumerate(search_results, 1):
                    distance = result.get('distance', 0.0)
                    similarity = (1 - distance) * 100  # Convert distance to similarity percentage
                    preview = result['text'][:120] + "..." if len(result['text']) > 120 else result['text']
                    print(f"{i}. [{similarity:.1f}% —Å—Ö–æ–∂–µ—Å—Ç—å] {preview}")
                    log.debug(f"Result {i}: distance={distance:.3f}, text='{result['text'][:50]}...'")
            
        except KeyboardInterrupt:
            log.info("Interactive session interrupted by user (Ctrl+C)")
            print("

üëã –°–µ—Å—Å–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            log.error(f"Error in interactive session: {e}")
            print(f"
‚ùå –û—à–∏–±–∫–∞: {e}")

def test_persistent_rag(rag_system: Dict[str, Any], config: dict):
    """Test the persistent RAG system with sample questions."""
    
    log.info("Starting persistent RAG system test")
    print_section_header("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê")
    
    test_questions = [
        "–°–∫–æ–ª—å–∫–æ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–µ—Ä–æ–π –≤–∏–¥–µ–æ?",
        "–ß–µ–º –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –≥–µ—Ä–æ–π?", 
        "–ö–∞–∫ –æ–Ω –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–µ–Ω—å–≥–∏?",
        "–ß—Ç–æ –æ–Ω –≥–æ–≤–æ—Ä–∏—Ç –æ —Ä–∞–±–æ—Ç–µ?",
        "–ö—Ç–æ —ç—Ç–æ—Ç —á–µ–ª–æ–≤–µ–∫?"
    ]
    
    max_results = config['search']['max_results']
    
    for i, question in enumerate(test_questions, 1):
        log.info(f"Testing question {i}: '{question}'")
        print(f"
{i}. –í–æ–ø—Ä–æ—Å: {question}")
        
        # Perform semantic search
        search_results = semantic_search(question, rag_system, top_k=max_results)
        answer = generate_rag_answer(question, search_results, rag_system["full_text"])
        
        print(f"   –û—Ç–≤–µ—Ç: {answer[:100]}...")
        
        if search_results:
            print(f"   üìä –ù–∞–π–¥–µ–Ω–æ: {len(search_results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            best_similarity = (1 - search_results[0].get('distance', 1.0)) * 100
            print(f"   üéØ –õ—É—á—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {best_similarity:.1f}%")
    
    log.info("Persistent RAG system test completed")
    print("
‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

@log.catch
def main():
    """Main function for persistent RAG demo."""
    log.info("=== MAIN FUNCTION STARTED ===")
    log.info(f"Script path: {Path(__file__)}")
    log.info(f"Current working directory: {Path.cwd()}")
    log.info(f"Python path: {sys.path[:3]}...")  # First 3 entries
    
    print_session_header("PERSISTENT RAG DEMO - CHROMADB –í–ï–ö–¢–û–†–ù–û–ï –•–†–ê–ù–ò–õ–ò–©–ï")
    
    try:
        # Load configuration
        log.info("Loading configuration")
        config = load_config()
        
        # Create persistent RAG system
        log.info("Creating persistent RAG system")
        rag_system = create_persistent_rag_system(config)
        if not rag_system:
            log.error("Failed to create persistent RAG system")
            print("‚ùå Failed to create persistent RAG system")
            return 1
        
        log.info("Persistent RAG system created successfully")
        log.debug(f"RAG system metadata: {rag_system['metadata']}")
        
        # Test the system
        test_persistent_rag(rag_system, config)
        
        # Start interactive session
        log.info("Starting interactive persistent Q&A session")
        interactive_persistent_qa(rag_system, config)
        
        log.info("=== MAIN FUNCTION COMPLETED ===")
        return 0
        
    except Exception as e:
        log.error(f"Error in main function: {e}")
        print(f"‚ùå Error: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
