#!/usr/bin/env python3
"""Enhanced RAG demo using full project pipeline and features."""

import sys
from pathlib import Path
from typing import Dict, Any, Optional
import json

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from loguru import logger
except ImportError:
    # Fallback logger for testing without loguru
    class FallbackLogger:
        def debug(self, msg): print(f"DEBUG: {msg}")
        def info(self, msg): print(f"INFO: {msg}")
        def warning(self, msg): print(f"WARNING: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
        def catch(self, func): return func
    logger = FallbackLogger()

# Mock loguru for project modules that depend on it
import sys
if 'loguru' not in sys.modules:
    sys.modules['loguru'] = type(sys)('loguru')
    sys.modules['loguru'].logger = logger

try:
    from llm_rag_yt.pipeline import RAGPipeline
    from llm_rag_yt.config.settings import Config, get_config
    FULL_PIPELINE_AVAILABLE = True
    logger.info("Full RAG pipeline available")
except ImportError as e:
    logger.warning(f"Full pipeline not available: {e}")
    FULL_PIPELINE_AVAILABLE = False
    # Create dummy classes for type hints when full pipeline unavailable
    class RAGPipeline:
        pass
    
from llm_rag_yt.utils import setup_logger, load_transcription_text, create_rag_metadata
from user_testing.config import get_demo_config




def create_enhanced_rag_system() -> Optional[RAGPipeline]:
    """Create an enhanced RAG system using the full project pipeline."""
    demo_config = get_demo_config()
    
    logger.info("Starting enhanced RAG system creation")
    print("üîß Creating enhanced RAG system with full pipeline...")

    # Load transcription using utility function
    logger.debug("Loading transcription text")
    transcript_text = load_transcription_text(demo_config.transcript_paths)
    if not transcript_text:
        logger.error("No transcript text loaded, aborting RAG creation")
        return None

    logger.info(f"Transcript loaded successfully: {len(transcript_text)} characters")

    try:
        # Initialize full RAG pipeline with project configuration
        logger.debug("Initializing RAG pipeline")
        project_config = get_config()
        rag_pipeline = RAGPipeline(project_config)
        
        logger.info("RAG pipeline initialized successfully")
        print("‚úÖ RAG pipeline components loaded")

        # Create mock transcription data structure that pipeline expects
        mock_file_id = "demo_transcript"
        mock_transcription = {
            mock_file_id: {
                "full_text": transcript_text,
                "segments": [],  # Not needed for this demo
                "language": "ru"
            }
        }

        # Process transcript through the text processor
        logger.info("Processing transcript through pipeline")
        print("üìù Processing transcript with TextProcessor...")
        
        normalized_texts = rag_pipeline.text_processor.process_transcriptions(
            mock_transcription, 
            project_config.chunk_size, 
            project_config.chunk_overlap
        )
        
        # Create chunks with proper metadata
        chunks = rag_pipeline.text_processor.create_chunks(
            normalized_texts, 
            project_config.chunk_size, 
            project_config.chunk_overlap
        )
        
        logger.info(f"Created {len(chunks)} chunks with metadata")
        print(f"‚úÖ Created {len(chunks)} chunks with metadata")

        # Store chunks in vector database
        logger.info("Storing chunks in vector database")
        print("üóÉÔ∏è Indexing chunks in vector database...")
        
        rag_pipeline.vector_store.upsert_chunks(rag_pipeline.encoder, chunks)
        
        logger.info("Chunks successfully stored in vector database")
        print("‚úÖ Vector database ready for queries")

        # Save processing artifacts
        logger.debug("Saving processing artifacts")
        rag_pipeline._save_artifacts("demo_normalized", {"normalized_texts": normalized_texts})
        
        # Store demo metadata
        metadata = create_rag_metadata(
            title=demo_config.video_title,
            chunk_count=len(chunks),
            transcript_length=len(transcript_text),
            method="full_pipeline",
            vector_store_collection=project_config.collection_name
        )
        
        with open(project_config.artifacts_dir / "demo_metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info("Enhanced RAG system created successfully")
        return rag_pipeline

    except Exception as e:
        logger.error(f"Failed to create enhanced RAG system: {e}")
        print(f"‚ùå Failed to create enhanced RAG system: {e}")
        return None


def create_fallback_rag_system() -> Optional[Dict[str, Any]]:
    """Create a fallback RAG system using basic text processing when full pipeline unavailable."""
    demo_config = get_demo_config()
    
    logger.info("Creating fallback RAG system")
    print("üîß Creating fallback RAG system (basic text processing)...")

    # Load transcription using utility function
    logger.debug("Loading transcription text")
    transcript_text = load_transcription_text(demo_config.transcript_paths)
    if not transcript_text:
        logger.error("No transcript text loaded, aborting RAG creation")
        return None

    logger.info(f"Transcript loaded successfully: {len(transcript_text)} characters")

    try:
        from llm_rag_yt.text.processor import TextProcessor
        from llm_rag_yt.config.settings import get_config
        
        project_config = get_config()
        text_processor = TextProcessor()
        
        # Process the transcript
        logger.info("Starting transcript processing with fallback method")
        print("üìù Processing transcript into chunks (fallback mode)...")

        normalized_text = text_processor.normalize_text(transcript_text)
        processed_chunks = text_processor.split_into_chunks(
            normalized_text, 
            chunk_size=project_config.chunk_size, 
            overlap=project_config.chunk_overlap
        )

        logger.info(f"Successfully created {len(processed_chunks)} text chunks")
        print(f"‚úÖ Created {len(processed_chunks)} text chunks")

        metadata = create_rag_metadata(
            title=demo_config.video_title,
            chunk_count=len(processed_chunks),
            transcript_length=len(transcript_text),
            method="fallback_text_processor"
        )
        
        rag_data = {
            "chunks": processed_chunks,
            "full_text": transcript_text,
            "metadata": metadata,
        }

        logger.info("Fallback RAG data structure created successfully")
        return rag_data

    except Exception as e:
        logger.error(f"Failed to create fallback RAG system: {e}")
        print(f"‚ùå Failed to create fallback RAG system: {e}")
        return None


def interactive_fallback_qa(rag_data: Dict[str, Any]):
    """Run fallback interactive Q&A session using basic search."""
    from llm_rag_yt.utils import simple_search, generate_simple_answer
    
    logger.info("Starting fallback interactive Q&A session")
    
    print("\n" + "=" * 70)
    print("ü§ñ –ë–ê–ó–û–í–ê–Ø –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–ê–Ø –°–ï–°–°–ò–Ø –í–û–ü–†–û–°–û–í –ò –û–¢–í–ï–¢–û–í")
    print("=" * 70)
    print(f"üìπ –í–∏–¥–µ–æ: {rag_data['metadata']['title']}")
    print(f"üìä –î–∞–Ω–Ω—ã–µ: {rag_data['metadata']['chunk_count']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞")
    print("üí° –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –≤–∏–¥–µ–æ")
    print("‚å®Ô∏è –ö–æ–º–∞–Ω–¥—ã: 'quit'/'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ (–±–µ–∑ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)")
    print("=" * 70)

    while True:
        try:
            logger.debug("Waiting for user input...")
            question = input("\nü§î –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

            if question.lower() in ["quit", "exit", "–≤—ã—Ö–æ–¥", "q"]:
                logger.info("User requested to quit interactive session")
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break

            if not question:
                logger.debug("Empty question received")
                print("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")
                continue

            logger.info(f"Processing user question: '{question}'")
            print(f"\nüîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è: '{question}'")

            # Search for relevant chunks using utility function
            logger.debug("Starting chunk search")
            relevant_chunks = simple_search(question, rag_data["chunks"])

            # Generate answer using utility function
            logger.debug("Starting answer generation")
            answer = generate_simple_answer(
                question, relevant_chunks, rag_data["full_text"]
            )

            logger.info(f"Generated answer for question '{question}': '{answer[:50]}...'")

            print(f"\nüí¨ –û–¢–í–ï–¢ (–±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫):")
            print("-" * 50)
            print(answer)

            if relevant_chunks:
                logger.debug(f"Displaying {len(relevant_chunks)} relevant chunks")
                print(f"\nüìö –ù–ê–ô–î–ï–ù–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´ ({len(relevant_chunks)}):")
                print("-" * 50)
                for i, chunk in enumerate(relevant_chunks, 1):
                    preview = chunk[:120] + "..." if len(chunk) > 120 else chunk
                    print(f"{i}. {preview}")
                    logger.debug(f"Chunk {i}: '{chunk[:50]}...'")

        except KeyboardInterrupt:
            logger.info("Interactive session interrupted by user (Ctrl+C)")
            print("\n\nüëã –°–µ—Å—Å–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"Error in interactive session: {e}")
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")






def interactive_enhanced_qa(rag_pipeline: RAGPipeline):
    """Run enhanced interactive Q&A session using full RAG pipeline."""

    logger.info("Starting enhanced interactive Q&A session")
    
    # Get pipeline status for display
    status = rag_pipeline.get_status()
    collection_info = status["collection"]
    
    print("\n" + "=" * 70)
    print("ü§ñ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–ê–Ø –°–ï–°–°–ò–Ø –í–û–ü–†–û–°–û–í –ò –û–¢–í–ï–¢–û–í")
    print("=" * 70)
    print(f"üìπ –í–∏–¥–µ–æ: {get_demo_config().video_title}")
    print(f"üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è: {collection_info['name']} ({collection_info['count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
    print(f"üß† –ú–æ–¥–µ–ª—å: {rag_pipeline.config.openai_model}")
    print(f"üîç Embedding: {rag_pipeline.config.embedding_model.split('/')[-1]}")
    print("üí° –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –≤–∏–¥–µ–æ")
    print("‚å®Ô∏è –ö–æ–º–∞–Ω–¥—ã: 'quit'/'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞, 'status' –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã")
    print("=" * 70)

    while True:
        try:
            logger.debug("Waiting for user input...")
            question = input("\nü§î –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

            if question.lower() in ["quit", "exit", "–≤—ã—Ö–æ–¥", "q"]:
                logger.info("User requested to quit interactive session")
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if question.lower() in ["status", "—Å—Ç–∞—Ç—É—Å"]:
                logger.debug("User requested system status")
                status = rag_pipeline.get_status()
                print(f"\nüìä –°–¢–ê–¢–£–° –°–ò–°–¢–ï–ú–´:")
                print(f"  ‚Ä¢ –ö–æ–ª–ª–µ–∫—Ü–∏—è: {status['collection']['name']}")
                print(f"  ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {status['collection']['count']}")
                print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å –ò–ò: {rag_pipeline.config.openai_model}")
                print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {rag_pipeline.config.chunk_size} —Å–ª–æ–≤")
                print(f"  ‚Ä¢ –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {rag_pipeline.config.chunk_overlap} —Å–ª–æ–≤")
                print(f"  ‚Ä¢ Top-K: {rag_pipeline.config.top_k}")
                continue

            if not question:
                logger.debug("Empty question received")
                print("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")
                continue

            logger.info(f"Processing user question: '{question}'")
            print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å: '{question}'")

            # Query using full RAG pipeline with advanced features
            logger.debug("Querying RAG pipeline")
            result = rag_pipeline.query(
                question, 
                top_k=rag_pipeline.config.top_k
            )

            logger.info(f"Generated answer in {result['response_time']:.2f}s using {result['search_method']} search")

            print(f"\nüí¨ –û–¢–í–ï–¢ ({result['search_method']} –ø–æ–∏—Å–∫, {result['response_time']:.2f}—Å):")
            print("-" * 50)
            print(result["answer"])

            if result["sources"]:
                logger.debug(f"Displaying {len(result['sources'])} source documents")
                print(f"\nüìö –ò–°–¢–û–ß–ù–ò–ö–ò ({len(result['sources'])}):")
                print("-" * 50)
                for i, source in enumerate(result["sources"], 1):
                    text_preview = source['text'][:120] + "..." if len(source['text']) > 120 else source['text']
                    similarity = source.get('similarity', 'N/A')
                    chunk_id = source.get('metadata', {}).get('source_id', 'unknown')
                    print(f"{i}. [{chunk_id}] (sim: {similarity}) {text_preview}")
                    logger.debug(f"Source {i}: chunk_id={chunk_id}, similarity={similarity}")

        except KeyboardInterrupt:
            logger.info("Interactive session interrupted by user (Ctrl+C)")
            print("\n\nüëã –°–µ—Å—Å–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"Error in interactive session: {e}")
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")


def main():
    """Main function for enhanced RAG demo."""
    # Setup logger first
    setup_logger(script_name="enhanced_rag_demo")
    
    logger.info("=== ENHANCED RAG DEMO STARTED ===")
    logger.info(f"Script path: {Path(__file__)}")
    logger.info(f"Current working directory: {Path.cwd()}")
    logger.info(f"Python path: {sys.path[:3]}...")  # First 3 entries

    print("üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø RAG DEMO - –ü–û–õ–ù–´–ô PIPELINE –ò Q&A")
    print("=" * 60)

    # Try to create enhanced RAG system first
    if FULL_PIPELINE_AVAILABLE:
        logger.info("Attempting to create enhanced RAG system")
        rag_pipeline = create_enhanced_rag_system()
        
        if rag_pipeline:
            logger.info("Enhanced RAG system created successfully")
            
            # Show system status
            status = rag_pipeline.get_status()
            print(f"\n‚úÖ –ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞:")
            print(f"  ‚Ä¢ –ö–æ–ª–ª–µ–∫—Ü–∏—è: {status['collection']['name']} ({status['collection']['count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
            print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å –ò–ò: {rag_pipeline.config.openai_model}")
            print(f"  ‚Ä¢ Embedding –º–æ–¥–µ–ª—å: {rag_pipeline.config.embedding_model.split('/')[-1]}")

            # Show some examples from config
            config = get_demo_config()
            print("\nüß™ –ü–†–ò–ú–ï–†–´ –í–û–ü–†–û–°–û–í:")
            for q in config.example_questions:
                print(f"  ‚Ä¢ {q}")

            # Test one example using the full pipeline
            print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –í–û–ü–†–û–° –° –ü–û–õ–ù–´–ú PIPELINE:")
            test_question = "–°–∫–æ–ª—å–∫–æ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–µ—Ä–æ–π –≤–∏–¥–µ–æ?"
            print(f"–í–æ–ø—Ä–æ—Å: {test_question}")

            logger.info(f"Running test question through full pipeline: '{test_question}'")
            try:
                result = rag_pipeline.query(test_question, top_k=3)
                print(f"–û—Ç–≤–µ—Ç ({result['search_method']} –ø–æ–∏—Å–∫, {result['response_time']:.2f}—Å): {result['answer']}")
                print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(result['sources'])}")
                logger.info(f"Test question completed successfully in {result['response_time']:.2f}s")
            except Exception as e:
                logger.error(f"Test question failed: {e}")
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –≤–æ–ø—Ä–æ—Å–µ: {e}")
                print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ OPENAI_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

            # Start enhanced interactive session
            logger.info("Starting enhanced interactive Q&A session")
            interactive_enhanced_qa(rag_pipeline)

            logger.info("=== ENHANCED RAG DEMO COMPLETED ===")
            return 0

    # Fallback to basic system if enhanced fails
    logger.warning("Enhanced pipeline not available, falling back to basic system")
    print("\n‚ö†Ô∏è  –ü–æ–ª–Ω—ã–π pipeline –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É...")
    print("üí° –î–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: chromadb, sentence-transformers, openai, loguru")
    
    rag_data = create_fallback_rag_system()
    if not rag_data:
        logger.error("Failed to create any RAG system")
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å RAG —Å–∏—Å—Ç–µ–º—É")
        return 1

    logger.info("Fallback RAG system created successfully")
    print(f"\n‚úÖ –ë–∞–∑–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞:")
    print(f"  ‚Ä¢ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {rag_data['metadata']['chunk_count']}")
    print(f"  ‚Ä¢ –ú–µ—Ç–æ–¥: {rag_data['metadata'].get('method', 'unknown')}")

    # Show some examples from config
    config = get_demo_config()
    print("\nüß™ –ü–†–ò–ú–ï–†–´ –í–û–ü–†–û–°–û–í:")
    for q in config.example_questions:
        print(f"  ‚Ä¢ {q}")

    # Test one example using fallback
    print(f"\nüß™ –¢–ï–°–¢–û–í–´–ô –í–û–ü–†–û–° –° –ë–ê–ó–û–í–´–ú –ü–û–ò–°–ö–û–ú:")
    test_question = "–°–∫–æ–ª—å–∫–æ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–µ—Ä–æ–π –≤–∏–¥–µ–æ?"
    print(f"–í–æ–ø—Ä–æ—Å: {test_question}")

    logger.info(f"Running test question through fallback system: '{test_question}'")
    try:
        from llm_rag_yt.utils import simple_search, generate_simple_answer
        relevant_chunks = simple_search(test_question, rag_data["chunks"])
        answer = generate_simple_answer(test_question, relevant_chunks, rag_data["full_text"])
        print(f"–û—Ç–≤–µ—Ç (–±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫): {answer}")
        print(f"–§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(relevant_chunks)}")
        logger.info(f"Test question completed successfully")
    except Exception as e:
        logger.error(f"Test question failed: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –≤–æ–ø—Ä–æ—Å–µ: {e}")

    # Start fallback interactive session
    logger.info("Starting fallback interactive Q&A session")
    interactive_fallback_qa(rag_data)

    logger.info("=== FALLBACK RAG DEMO COMPLETED ===")
    return 0


if __name__ == "__main__":
    sys.exit(main())
